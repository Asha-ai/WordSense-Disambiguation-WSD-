{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import word_tokenize and sent_tokenize from nltk.tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mobile networks are complex and consist of many key network entities.', 'These entities then fall under the mobile core and mobile radio parts of the network.']\n"
     ]
    }
   ],
   "source": [
    "text=\"Mobile networks are complex and consist of many key network entities. These entities then fall under the mobile core and mobile radio parts of the network.\"\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sents=sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mobile', 'networks', 'are', 'complex', 'and', 'consist', 'of', 'many', 'key', 'network', 'entities', '.'], ['These', 'entities', 'then', 'fall', 'under', 'the', 'mobile', 'core', 'and', 'mobile', 'radio', 'parts', 'of', 'the', 'network', '.']]\n"
     ]
    }
   ],
   "source": [
    "words=[word_tokenize(sent) for sent in sents]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stopwords from the corpus and punctuation from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from string import punctuation\n",
    "customStopWords=set(stopwords.words('english')+list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mobile', 'networks', 'complex', 'consist', 'many', 'key', 'network', 'entities', 'These', 'entities', 'fall', 'mobile', 'core', 'mobile', 'radio', 'parts', 'network']\n"
     ]
    }
   ],
   "source": [
    "wordsWOStopwords=[word for word in word_tokenize(text) if word not in customStopWords]\n",
    "print(wordsWOStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Mobile', 'networks'), 7.606414586238348), (('complex', 'consist'), 7.606414586238348), (('consist', 'many'), 7.606414586238348), (('many', 'key'), 7.606414586238348), (('networks', 'complex'), 7.606414586238348), (('radio', 'parts'), 7.606414586238348), (('These', 'entities'), 4.833825863998566), (('core', 'mobile'), 4.833825863998566), (('entities', 'These'), 4.833825863998566), (('entities', 'fall'), 4.833825863998566), (('fall', 'mobile'), 4.833825863998566), (('key', 'network'), 4.833825863998566), (('mobile', 'core'), 4.833825863998566), (('mobile', 'radio'), 4.833825863998566), (('parts', 'network'), 4.833825863998566), (('network', 'entities'), 2.194669416526425)]\n"
     ]
    }
   ],
   "source": [
    "bgm    = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wordsWOStopwords)\n",
    "scored = finder.score_ngrams( bgm.likelihood_ratio  )\n",
    "print(scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.collocations import *\n",
    "# bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# finder = BigramCollocationFinder.from_words(wordsWOStopwords)\n",
    "# sorted(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\asha.ponnada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import LancasterStemmer from nltk then stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input', 'and', 'output', 'dev', ',', 'also', 'refer', 'to', 'as', \"'terminals'these\", 'provid', 'the', 'start', 'and', 'stop', 'point', 'of', 'al', 'commun', '.', 'a', 'telephon', 'is', 'an', 'exampl', 'of', 'a', 'termin', '.', 'in', 'comput', 'network', ',', 'thes', 'dev', 'ar', 'common', 'refer', 'to', 'as', \"'nodes\", \"'\", 'and', 'consist', 'of', 'comput', 'and', 'periph', 'devices..']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Input and output devices, also referred to as 'terminals'These provide the starting and stopping points of all communication. A telephone is an example of a terminal. In computer networks, these devices are commonly referred to as 'nodes' and consist of computer and peripheral devices..\"\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st=LancasterStemmer()\n",
    "stemmedWords=[st.stem(word) for word in word_tokenize(text2)]\n",
    "print(stemmedWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger is used to assign grammatical information of each word of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'NNP'),\n",
       " ('closed', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('closing', 'NN'),\n",
       " ('night', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('she', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mood', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('close', 'VB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synset\n",
    "WordNet is the lexical database i.e. dictionary for the English language, specifically designed\n",
    "for natural language processing.\n",
    "\n",
    "Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet.\n",
    "Synset instances are the groupings of synonymous words that express the same concept. Some of the \n",
    "words have only one Synset and some have several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset name :   good.n.01\n",
      "\n",
      "Synset meaning :  benefit\n",
      "\n",
      "Synset example :  ['for your own good', \"what's the good of worrying?\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "syn = wordnet.synsets('good')[0] \n",
    "  \n",
    "print (\"Synset name :  \", syn.name()) \n",
    "  \n",
    "# Defining the word \n",
    "print (\"\\nSynset meaning : \", syn.definition()) \n",
    "  \n",
    "# list of phrases that use the word in context \n",
    "print (\"\\nSynset example : \", syn.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.01') the lowest part of the musical range\n",
      "Synset('bass.n.02') the lowest part in polyphonic music\n",
      "Synset('bass.n.03') an adult male singer with the lowest voice\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
      "Synset('bass.n.06') the lowest adult male singing voice\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
      "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('bass'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation (WSD)\n",
    "Word Sense Disambiguation (WSD), has been a trending area of research in Natural Language\n",
    "Processing and Machine Learning. WSD is basically solution to the ambiguity which arises due \n",
    "to different meaning of words in different context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Approaches of WSD\n",
    "WSD APPROACHES: There are two approaches that are followed for Word Sense Disambiguation (WSD):\n",
    "        Machine-Learning Based approach and Knowledge Based approach. In Machine learning- based \n",
    "        approach, systems are trained to perform the task of word sense disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text Mining and Information Extraction (IE)\n",
    "In most of the applications, WSD is necessary to do accurate analysis of text. For example,\n",
    "WSD helps intelligent gathering system to do flagging of the correct words. \n",
    "\n",
    "Machine translation or MT is the most obvious application of WSD. In MT, Lexical choice for the \n",
    "words that have distinct translations for different senses, is done by WSD. The senses in MT \n",
    "are represented as words in the target language. Most of the machine translation systems do not\n",
    "use explicit WSD module.\n",
    "\n",
    "Lexicography\n",
    "WSD and lexicography can work together in loop because modern lexicography is corpusbased. \n",
    "With lexicography, WSD provides rough empirical sense groupings as well as statistically\n",
    "significant contextual indicators of sense.\n",
    "\n",
    "Information Retrieval (IR)\n",
    "Information retrieval (IR) may be defined as a software program that deals with the organization,\n",
    "storage, retrieval and evaluation of information from document repositories particularly textual \n",
    "information. The system basically assists users in finding the information they required but it does\n",
    "not explicitly return the answers of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We understand that words have different meanings based on the context of its usage in the sentence.\n",
    "If we talk about human languages, then they are ambiguous too because many words can be interpreted\n",
    "in multiple ways depending upon the context of their occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word sense disambiguation, in natural language processing (NLP), may be defined as the ability\n",
    "to determine which meaning of word is activated by the use of word in a particular context.\n",
    "Lexical ambiguity, syntactic or semantic, is one of the very first problem that any NLP system faces.\n",
    "Part-of-speech (POS) taggers with high level of accuracy can solve Word’s syntactic ambiguity.\n",
    "On the other hand, the problem of resolving semantic ambiguity is called WSD \n",
    "(word sense disambiguation). Resolving semantic ambiguity is harder than resolving syntactic \n",
    "ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Lesk\n",
    "Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset\n",
    "with the highest number of overlapping words between the context sentence and different\n",
    "definitions from each Synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition if lesk function\n",
    "lesk(context_sentence, ambiguous_word, pos=None, synsets=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "sense1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"),'bass')\n",
    "print(sense1, sense1.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
     ]
    }
   ],
   "source": [
    "sense2 = lesk(word_tokenize(\"This sea bass was really hard to catch\"),'bass')\n",
    "print(sense2, sense2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.01') the lowest part of the musical range\n",
      "Synset('bass.n.02') the lowest part in polyphonic music\n",
      "Synset('bass.n.03') an adult male singer with the lowest voice\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
      "Synset('bass.n.06') the lowest adult male singing voice\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
      "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n",
      "Synset('sea_bass.n.01')\n",
      "Synset('bass.s.01')\n",
      "Synset('sea_bass.n.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "for ss in wn.synsets('bass'):\n",
    "    print(ss,ss.definition())\n",
    "print(lesk('I went fishing fro some sea bass'.split(),'bass','n'))\n",
    "print(lesk('The bass line of the song is too weak'.split(),'bass','s'))\n",
    "print(lesk('Avishai cohen is an Israeli Jazz musician.He plays double bass and is also a composer'.split(),'bass',pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "third sentence where the bass is in the context of musical instrument, it is estimating the word as Synset('sea_bass.n.01) which is clearly not correct! Unfortunately, Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('plant.n.01') buildings for carrying on industrial labor\n",
      "Synset('plant.n.02') (botany) a living organism lacking the power of locomotion\n",
      "Synset('plant.n.03') an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience\n",
      "Synset('plant.n.04') something planted secretly for discovery by another\n",
      "Synset('plant.v.01') put or set (seeds, seedlings, or plants) into the ground\n",
      "Synset('implant.v.01') fix or set securely or deeply\n",
      "Synset('establish.v.02') set up or lay the groundwork for\n",
      "Synset('plant.v.04') place into a river\n",
      "Synset('plant.v.05') place something or someone in a certain position in order to secretly observe or deceive\n",
      "Synset('plant.v.06') put firmly in the mind\n",
      "Synset('plant.n.03')\n",
      "None\n",
      "Synset('plant.n.03')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "for i in wn.synsets('plant'):\n",
    "    print(i,i.definition())\n",
    "    \n",
    "print(lesk('The workers at the plant were overworked'.split(),'plant','n'))\n",
    "print(lesk('The plant was no longer bearing flowers'.split(),'plant','s'))\n",
    "print(lesk('The workers at the industrial plant were overworked'.split(),'plant',pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_sents = ['I went to the bank to deposit my money',\n",
    "'The river bank was full of dead fishes']\n",
    "\n",
    "plant_sents = ['The workers at the industrial plant were overworked',\n",
    "'The plant was no longer bearing flowers']\n",
    "\n",
    "\n",
    "from lesk import simple_lesk\n",
    "print()\"#TESTING simple_lesk() ...\")\n",
    "print (\"Context:\", bank_sents[0])\n",
    "answer = simple_lesk(bank_sents[0],'bank')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition:\",answer.definition)\n",
    "print()\n",
    "\n",
    "print (\"#TESTING simple_lesk() with POS ...\")\n",
    "print (\"Context:\", bank_sents[1])\n",
    "answer = simple_lesk(bank_sents[1],'bank','n')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition:\",answer.definition)\n",
    "print()\n",
    "\n",
    "print (\"#TESTING simple_lesk() with POS and stems ...\")\n",
    "print (\"Context:\", plant_sents[0])\n",
    "answer = simple_lesk(plant_sents[0],'plant','n', True)\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition:\",answer.definition)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pywsd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install pywsd  \n",
    "!pip install pywsd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Python code to remove word ambiguity using the Lesk algorithm. \n",
    "For example, in the sentences below, the word “bank” has different meanings based on the context \n",
    "of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Text1 = 'I went to the bank to deposit my money'\n",
    "Text2 = 'The river bank was full of dead fishes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The Lesk algorithm is the seminal dictionary-based method.\n",
    "This is the definition from Wikipedia: \"It is based on the hypothesis that words used together in \n",
    "    text are related to each other and that the relation can be observed in the definitions of the\n",
    "    words and their senses. Two (or more) words are disambiguated by finding the pair of dictionary\n",
    "    senses with the greatest word overlap in their dictionary definitions. It searches for the\n",
    "    shortest path between two words: the second word is iteratively searched among the definitions\n",
    "        of every semantic variant of the first word, then among the definitions of every semantic \n",
    "        variant of each word in the previous definitions and so on.Finally, the first word is\n",
    "        disambiguated by selecting the semantic variant which minimizes the distance from the \n",
    "        first to the second word.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Basically, the context is chosen from meaning of the nearest words. Following is the simplified \n",
    "pictorial representation of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Let's see the code to implement the Lesk algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "First install the library pywsd - python implementation of Word Sense Disambiguation (WSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-1: The river bank was full of dead fishes\n",
      "Sense: Synset('bank.n.01')\n",
      "Definition :  sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#Import functions  \n",
    "from pywsd.lesk import simple_lesk  \n",
    "sentences = ['The river bank was full of dead fishes','I went to the bank to deposit my money']  \n",
    "# calling the lesk function and printing results for both the sentences  \n",
    "print (\"Context-1:\", sentences[0])  \n",
    "answer = simple_lesk(sentences[0],'bank')  \n",
    "print (\"Sense:\", answer)  \n",
    "print (\"Definition : \", answer.definition()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-2: I went to the bank to deposit my money\n",
      "Sense: Synset('depository_financial_institution.n.01')\n",
      "Definition :  a financial institution that accepts deposits and channels the money into lending activities\n"
     ]
    }
   ],
   "source": [
    "#Import functions  \n",
    "from pywsd.lesk import simple_lesk  \n",
    "sentences = ['The river bank was full of dead fishes','I went to the bank to deposit my money']  \n",
    "# calling the lesk function and printing results for both the sentences  \n",
    "print (\"Context-2:\", sentences[1])  \n",
    "answer = simple_lesk(sentences[1],'bank')  \n",
    "print (\"Sense:\", answer)  \n",
    "print (\"Definition : \", answer.definition()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-0: Every now and then users will report bugs in Telegram\n",
      "Sense: Synset('wiretap.v.01')\n",
      "Definition :  tap a telephone or telegraph wire to get information\n"
     ]
    }
   ],
   "source": [
    "from pywsd.lesk import simple_lesk  \n",
    "sentences = ['Every now and then users will report bugs in Telegram','I have seen a bug in my computer code']  \n",
    "# calling the lesk function and printing results for both the sentences  \n",
    "print (\"Context-0:\", sentences[0])  \n",
    "answer = simple_lesk(sentences[0],'bug')  \n",
    "print (\"Sense:\", answer)  \n",
    "print (\"Definition : \", answer.definition()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-0: Every now and then users will report bugs in Telegram\n",
      "Sense: Synset('wiretap.v.01')\n",
      "Definition :  tap a telephone or telegraph wire to get information\n"
     ]
    }
   ],
   "source": [
    "from pywsd.lesk import simple_lesk  \n",
    "sentences = ['Every now and then users will report bugs in Telegram','I have seen a bug in my computer code']  \n",
    "# calling the lesk function and printing results for both the sentences  \n",
    "print (\"Context-0:\", sentences[0])  \n",
    "answer = simple_lesk(sentences[0],'bug')  \n",
    "print (\"Sense:\", answer)  \n",
    "print (\"Definition : \", answer.definition()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-0: Head aches maybe sign of stress\n",
      "Sense: Synset('wiretap.v.01')\n",
      "Definition :  tap a telephone or telegraph wire to get information\n"
     ]
    }
   ],
   "source": [
    "from pywsd.lesk import simple_lesk  \n",
    "sentences = ['Head aches maybe sign of stress','you forgot to sign the cheque']  \n",
    "# calling the lesk function and printing results for both the sentences  \n",
    "print (\"Context-0:\", sentences[0])  \n",
    "answer = simple_lesk(sentences[0],'bug')  \n",
    "print (\"Sense:\", answer)  \n",
    "print (\"Definition : \", answer.definition()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-0: Head aches maybe sign of stress\n",
      "Sense: Synset('signboard.n.01')\n",
      "Definition :  structure displaying a board on which advertisements can be posted\n"
     ]
    }
   ],
   "source": [
    "from pywsd.lesk import simple_lesk  \n",
    "sentences = ['Head aches maybe sign of stress','you forgot to sign the cheque']  \n",
    "# calling the lesk function and printing results for both the sentences  \n",
    "print (\"Context-0:\", sentences[0])  \n",
    "answer = simple_lesk(sentences[0],'sign')  \n",
    "print (\"Sense:\", answer)  \n",
    "print (\"Definition : \", answer.definition()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
